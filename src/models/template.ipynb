{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if os.environ.get(\"USE_GPU\"):\n",
    "    import setGPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import timeit\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interp\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc, f1_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "data_base = \"/home/mantydze/data\" # no trailing slash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if os.environ.get(\"USE_EOS\"):\n",
    "    import getpass\n",
    "    \n",
    "    data_base = \"/eos/user/m/mantydze/data\" # no trailing slash\n",
    "    \n",
    "    #Authenticate in order to get permission for eos\n",
    "    os.system(\"echo %s | kinit\" % getpass.getpass())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "\n",
    "filename = \"{data_base}/JetHT2016_single/JetHT2016.h5\".format(data_base=data_base)\n",
    "\n",
    "dset = None\n",
    "\n",
    "with h5py.File(filename, \"r\") as f:\n",
    "    dset = f[\"JetHT2016\"][:] \n",
    "        \n",
    "print(dset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "\n",
    "# Feature 2807 - Run number\n",
    "# Feature 2808 - Lumisection number\n",
    "\n",
    "# #sort by run number and lumisection\n",
    "# dset = dset[np.lexsort((dset[:, 2808], dset[:, 2807]))]\n",
    "\n",
    "# Features\n",
    "X = dset[:10000, :2807] # 2807 features\n",
    "\n",
    "# Target\n",
    "y = dset[:10000, 2812]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "\n",
    "X = StandardScaler(copy=False).fit_transform(X) # copy=False reduce memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Class balance\n",
    "def print_unique(values):\n",
    "    unique, counts = np.unique(values, return_counts=True)\n",
    "\n",
    "    for cls, cnt in zip(unique, counts):\n",
    "        print(\"Class [%d] Count [%d]\" % (cls, cnt))\n",
    "        \n",
    "print_unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def print_split_info(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # Train class balance\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    train_balance = {int(k): v for k, v in zip(unique, counts)}\n",
    "    \n",
    "    # Test class balance\n",
    "    unique, counts = np.unique(y_test, return_counts=True)\n",
    "    test_balance = {int(k): v for k, v in zip(unique, counts)}\n",
    "    \n",
    "    # Train class weights\n",
    "    classes = np.unique(y_train)\n",
    "    weights = class_weight.compute_class_weight('balanced', classes, y_train)\n",
    "    cw = {int(k): round(v, 2) for k, v in zip(classes, weights)}\n",
    "    \n",
    "    print(\"Train: size %d\\t balance %s\\t weights %s\" % (len(X_train), train_balance, cw))\n",
    "    print(\"Test : size %d\\t balance %s\" % (len(X_test), test_balance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import model related libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(X_train, y_train, verbose=True, **options):\n",
    "    \"\"\" Returns trained model\n",
    "    \"\"\"\n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Training\")\n",
    "    \n",
    "    # Calculate class weights of Training set\n",
    "    classes = np.unique(y_train)\n",
    "    weights = class_weight.compute_class_weight('balanced', classes, y_train)\n",
    "    cw = {int(cls): weight for cls, weight in zip(classes, weights)}\n",
    "    \n",
    "    # Model code starts here\n",
    "    ############\n",
    "    \n",
    "    model = None\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    ############\n",
    "    # Model code ends here\n",
    "    \n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    if verbose:\n",
    "        print(\"Train time: %.1f s\" % (elapsed))\n",
    "        \n",
    "    secs.append(elapsed)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def evaluate(model, X_test, y_test, verbose=True):\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Evaluating\")\n",
    "        \n",
    "    # Accuracy and f1_score\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    accs.append(acc)\n",
    "    f1s.append(f1)\n",
    "    \n",
    "    roc_auc = -1\n",
    "    if acc < 1:\n",
    "\n",
    "        # ROC AUC\n",
    "        y_probas = model.predict_proba(X_test)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_probas)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        aucs.append(roc_auc)\n",
    "        fprs.append(fpr)\n",
    "        tprs.append(tpr)\n",
    "    \n",
    "    if verbose:\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        print(\"acc: %.3f | f1: %.3f | auc: %.3f\" % (acc, f1, roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "n_splits = 10\n",
    "\n",
    "accs = []\n",
    "f1s = []\n",
    "\n",
    "fprs = []\n",
    "tprs = []\n",
    "aucs = []\n",
    "secs = []\n",
    "\n",
    "skf = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.2)\n",
    "\n",
    "for index, (train_indices, test_indices) in enumerate(skf.split(X, y)):\n",
    "    print(\"Fold %d / %d\" % ((index + 1), n_splits))\n",
    "    \n",
    "    # Generate batches from indices\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "    \n",
    "    print_split_info(X_train, y_train, X_test, y_test)\n",
    "    model = train(X_train, y_train)\n",
    "    \n",
    "    evaluate(model, X_test, y_test)\n",
    "    \n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d = dict(acc=np.array(accs), f1=np.array(f1s), auc=np.array(aucs), sec=np.array(secs))\n",
    "scores = pd.DataFrame.from_dict(d, orient=\"index\")\n",
    "scores[\"mean\"] = scores.mean(axis=1)\n",
    "scores[\"std\"] = scores.std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(scores.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores.to_csv(\"scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "for i, (fpr, tpr) in enumerate(zip(fprs, tprs)):\n",
    "    plt.plot(fpr, tpr, label=\"Fold %d (auc: %.3f)\" % ((i+1), aucs[i]), alpha=0.4)\n",
    "\n",
    "tprs2 = []\n",
    "\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "for fpr, tpr in zip(fprs, tprs):\n",
    "    tprs2.append(interp(mean_fpr, fpr, tpr))\n",
    "    \n",
    "mean_tpr = np.mean(tprs2, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, \n",
    "         color=\"b\",\n",
    "         linestyle=\":\",\n",
    "         linewidth=4,\n",
    "         label=r\"Mean ROC (AUC = %0.3f $\\pm$ %0.3f)\" % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.9)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=.5)\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train = X_test = y_train = y_test = None # Release memory\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluate(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "joblib.dump(model, \"model.dat\")\n",
    "\n",
    "# Scores (acc, f1, auc)\n",
    "with open(\"scores.json\", \"w\") as outfile:\n",
    "    data = scores.to_dict()\n",
    "    json.dump(data, outfile, indent=4)\n",
    "\n",
    "# Mean TPR, FPR, AUC, AUC STD\n",
    "with open(\"roc.json\", \"w\") as outfile:\n",
    "    data = {\n",
    "        \"fpr\": mean_fpr.tolist(), \n",
    "        \"tpr\":mean_tpr.tolist(), \n",
    "        \"auc\": mean_auc, \n",
    "        \"std\": std_auc\n",
    "    }\n",
    "    json.dump(data, outfile, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
